{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"self_made_transformer.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNVfkho4zIpUBpwuc5FK5Z4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"gNMvsU9f053J","executionInfo":{"status":"ok","timestamp":1619746098215,"user_tz":-540,"elapsed":3799,"user":{"displayName":"조원준/학생/컴퓨터공학","photoUrl":"","userId":"15114687070600565911"}}},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","'''\n","N : batch_size\n","T : sequence_length\n","h : num_head\n","H : d_model (단어의 임베딩된 차원)\n","d : H // h\n","'''\n","class ScaledDotProductAttention(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","  def forward(self, query, key, value, mask=None):\n","    '''\n","    query : 비교주체 / (batch_size, sequence_length, num_head, head_dim) = (N, T, h, d)\n","    key   : 비교상대 / (batch_size, sequence_length, num_head, head_dim) = (N, T, h, d)\n","    value : 가중치가 곱해질 값들 / (batch_size, sequence_length, num_head, head_dim) = (N, T, h, d)\n","    mask  :\n","    '''\n","    _, T, h, H = query.shape\n","\n","    # query 와 key 를 matmul 하여 가중치(유사도) a 를 구한다. (각각의 word 마다 자신이 포함된 sequence 에서 어떤 word 와 유사한 지(= 어떤 word 를 attend 하는 지)를 나타냄.)\n","    a = query.transpose(1, 2).matmul(key) / np.sqrt(H) # gradient vanishing problem 방지하고자 scale / (N, h, T, d) @ (N, h, d, T) -> (N, h, T, T)\n","\n","    if mask is not None: # TODO : mask 값 확인해보기\n","      a.masked_fill(mask, 1e-9) # mask 가 True 인 경우 1e-9 로. (softmax 값 0으로.)\n","\n","    # 가중치 a 를 0~1 사이 값 & 총합 1 로 scale\n","    a = nn.Softmax(a)\n","\n","    # 가중치 a 와 value 를 matmul 하여 변환(번역)을 수행하는 데 필요한 정보가 담긴 맥락벡터 c 를 구한다. (어떤 word)\n","    c = a.matmul(value.transpose(1,2)) # (N, h, T, T) @ (N, h, T, d) -> (N, h, T, d)\n","\n","    return c\n","\n","class MultiHeadAttention(nn.Module):\n","  def __init__(self, batch_size, sequence_length, d_model, num_head, drop_ratio):\n","    super().__init__()\n","    self.N = batch_size\n","    self.T = sequence_length\n","    self.H = d_model\n","    self.h = num_head\n","    self.d = d_model // num_head # d = H * h\n","\n","    # paper(d_model -> dmodel/h) 과 달리 for 문을 사용하지 않기 위해 Linear(d_model->d_model) 로 하고 view 로 나누어 한번에 계산.\n","    self.qLinear = nn.Linear(d_model, d_model) # query linear\n","    self.kLinear = nn.Linear(d_model, d_model) # key linear\n","    self.vLinear = nn.Linear(d_model, d_model) # value linear\n","    self.lLinear = nn.Linear(d_model, d_model) # last linear\n","    self.dropout = nn.Dropout(drop_ratio)\n","\n","  def forward(self, query, key, value): # respective size : (N, T, H) #TODO : mask 추가.\n","    # multi head attention 을 위해 d_model 을 head 수 만큼 slice \n","    multi_query = self.qLinear(query).view(self.N, self.T, self.h, self.d)\n","    multi_key   = self.kLinear(key).view(self.N, self.T, self.h, self.d)\n","    multi_value = self.vLinear(value).view(self.N, self.T, self.h, self.d)\n","                         \n","    c = ScaledDotProductAttention(multi_query, multi_key, multi_value) # (N, h, T, d)\n","    c = c.transpose(1, 2).view(self.N, self.T, -1) # concat (N, T, h, d) -> (N, T, h * d) = (N, T, H)\n","\n","    output = self.lLinear(c) # (N, T, H)\n","    \n","    return self.dropout(output) # apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized\n","\n","class FeedForward(nn.Module):\n","  def __init__(self, d_model, inner_layer_dim, drop_ratio):\n","    '''\n","    inner_layer_dim : 중간 은닉층 dimension\n","    '''\n","    super().__init__()\n","    self.linear1 = nn.Linear(d_model, inner_layer_dim)\n","    self.linear2 = nn.Linear(inner_layer_dim, d_model)\n","    self.dropout = nn.Dropout(drop_ratio)\n","\n","  def forward(self, x):\n","    output = self.linear2(self.linear1(x))\n","    return self.dropout(output) # apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized\n","    \n","class EncoderLayer(nn.Module):\n","  def __init__(self, batch_size, sequence_length, d_model, num_head, inner_layer_dim, drop_ratio):\n","    super().__init__()\n","    self.layerNorm1 = nn.LayerNorm(d_model)\n","    self.layerNorm2 = nn.LayerNorm(d_model)\n","    self.multiHeadAttention = MultiHeadAttention(batch_size, sequence_length, d_model, num_head, drop_ratio)\n","    self.feedForward = FeedForward(d_model, inner_layer_dim, drop_ratio)\n","\n","  def forward(self, src):\n","    x = self.layerNorm1(src + self.multiHeadAttention(src, src, src)) # src = query = key = value / residual & layernorm\n","    output = self.layerNorm2(x + self.feedForward(x)) # residual & layernorm\n","    return output\n","\n","class Encoder(nn.Module):\n","  def __init__(self, batch_size, sequence_length, d_model, num_head, inner_layer_dim, drop_ratio, iter):\n","    '''\n","    iter : encoder layer 반복 수\n","    '''\n","    super().__init__()\n","    self.encoderLayers = nn.ModuleList([EncoderLayer(batch_size, sequence_length, d_model, num_head, inner_layer_dim, drop_ratio) for _ in range(iter)]) # iter 만큼 encoderLayer 반복\n","\n","  def forward(self, src):\n","    for encoderLayer in self.encoderLayers:\n","      src = encoderLayer(src)\n","\n","    return src # encoder 의 출력 값이자 모든 decoder layer 에 query, key 로 입력될 값.\n","\n","class DecoderLayer(nn.Module):\n","  def __init__(self, batch_size, sequence_length, d_model, num_head, inner_layer_dim, drop_ratio):\n","    super().__init__()\n","    self.layerNorm1 = nn.LayerNorm(d_model)\n","    self.layerNorm2 = nn.LayerNorm(d_model)\n","    self.layerNorm3 = nn.LayerNorm(d_model)\n","    self.multiHeadAttention1 = MultiHeadAttention(batch_size, sequence_length, d_model, num_head, drop_ratio)\n","    self.multiHeadAttention2 = MultiHeadAttention(batch_size, sequence_length, d_model, num_head, drop_ratio)\n","    self.feedForward = FeedForward(d_model, inner_layer_dim, drop_ratio)\n","\n","  def forward(self, src, enc_des):\n","    '''\n","    src     : decoder 에 입력되는 값 (src = query = key = value) / (N, T, H)\n","    enc_des : encoder 에서 출력된 값 / (N, T, H)\n","    '''\n","    # TODO masking 추가. \n","    query = self.layerNorm1(src + self.multiHeadAttention1(src, src, src))\n","\n","    # the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder\n","    x = self.layerNorm2(query + self.multiHeadAttention1(query, enc_des, enc_des))\n","\n","    output = self.layerNorm3(x + self.feedForward(x))\n","\n","    return output\n","    \n","class Decoder(nn.Module):\n","  def __init__(self, batch_size, sequence_length, d_model, num_head, inner_layer_dim, drop_ratio, iter):\n","    super().__init__()\n","    self.decoderLayers = nn.ModuleList([DecoderLayer(batch_size, sequence_length, d_model, num_head, inner_layer_dim, drop_ratio) for _ in range(iter)]) # iter 만큼 encoderLayer 반복\n","\n","  def forward(self, src, enc_des):\n","    for decoderLayer in self.decoderLayers:\n","      src = decoderLayer(src, enc_des)\n","\n","    return src\n","\n","class PositionalEncoding(nn.Module):\n","  def __init__(self, vocab_size, d_model):\n","    super().__init__()\n","    self.Embedding()\n","\n","  def forward(self, src):\n","    pass\n","    \n","class Transformer(nn.Module):\n","  def __init__(self, batch_size, sequence_length, d_model, num_head, inner_layer_dim, drop_ratio, iter, inputs_vocab_size, outputs_vocab_size):\n","    '''\n","    inputs_vocab_size : encoder 에 입력되는 dataset 의 vcoab 수\n","    outputs_vocab_size : decoder 에 입력되는 dataset 의 vocab 수\n","    '''\n","    super().__init__()\n","    self.scale = torch.sqrt(d_model)\n","\n","    # word embedding\n","    self.inputEmbedding = nn.Embedding(inputs_vocab_size, d_model)\n","    self.outputEmbedding = nn.Embedding(outputs_vocab_size, d_model) # TODO : multiply those weights by root(d_model)\n","\n","    # positional embedding\n","    self.encPositionalEncoding = PositionalEncoding(inputs_vocab_size, d_model)\n","    self.decPositionalEncoding = PositionalEncoding(outputs_vocab_size, d_model)\n","\n","    self.encoder = Encoder(batch_size, sequence_length, d_model, num_head, inner_layer_dim, drop_ratio, iter)\n","    self.decoder = Decoder(batch_size, sequence_length, d_model, num_head, inner_layer_dim, drop_ratio, iter)\n","\n","    self.linear = nn.Linear(d_model, outputs_vocab_size)\n","\n","  def forward(self, inputs, outputs):\n","\n","    enc_src = self.inputEmbedding(inputs) * self.scale + self.encPositionalEncoding(inputs)\n","    enc_des = self.encoder(enc_src)\n","\n","    dec_src = self.outputEmbedding(outputs) * self.scale + self.encPositionalEncoding(outputs)\n","    output = self.decoder(dec_src, enc_des)\n","\n","    output_prob = nn.Softmax(self.linear(output)) # we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation\n","    return output_prob                            # 두개 embedding layer 는 shape 가 다른거 같은데.. output embedding 하고만 겹치지 않나?\n","                                                  # 번역 할 때는 언어마다 vocab 수가 달라져서 shape 달라지고 한 언어로 문장 생성할 때는 vocab 수가 같은데.. 어찌해야하는가?\n","                                                  # vocab 수가 같을 수도 다를 수도 있으니 일단은 case 를 나눠서 같은 경우 세 가중치 공유, 다른 경우 output 쪽만 공유하도록 해보자.\n","    "],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"X0XbQr6zLEYs"},"source":[""],"execution_count":null,"outputs":[]}]}