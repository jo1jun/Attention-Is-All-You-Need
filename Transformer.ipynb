{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Transformer",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jo1jun/Transformer/blob/main/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyKo5seMwjFs"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "dtype = torch.long"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKyDkhQ4UNbF"
      },
      "source": [
        "# Multi Head Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohBIfgOJiL0a"
      },
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model # embed dimension\n",
        "        self.nhead = nhead # head 수\n",
        "        self.head_dim = d_model // nhead # head 마다의 dimension\n",
        "\n",
        "        # paper 보면 d_model * d_k 이런 shape 인데, 실제 구현에서는 d_model * d_model 하고 나온 것을 쪼개서 multi head self attention 에 들어가게 함.\n",
        "        self.qLinear = nn.Linear(d_model, d_model) # query lineqr\n",
        "        self.kLinear = nn.Linear(d_model, d_model) # key linear\n",
        "        self.vLinear = nn.Linear(d_model, d_model) # value linear\n",
        "        self.oLinear = nn.Linear(d_model, d_model) # output linear\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    def forward(self, query, key, value, mask = None):\n",
        "\n",
        "        batch_size = query.shape[0]\n",
        "        query_len = query.shape[1]\n",
        "        value_len = key_len = key.shape[1]\n",
        "\n",
        "        # query: [batch_size, query_len, d_model]\n",
        "        # key: [batch_size, key_len, d_model]\n",
        "        # value: [batch_size, value_len, d_model]\n",
        " \n",
        "        Q = self.qLinear(query)\n",
        "        K = self.kLinear(key)\n",
        "        V = self.vLinear(value)\n",
        "\n",
        "        # Q: [batch_size, query_len, d_model]\n",
        "        # K: [batch_size, key_len, d_model]\n",
        "        # V: [batch_size, value_len, d_model]\n",
        "\n",
        "        # d_model = nhead * head_dim\n",
        "        # head 수로 distribute\n",
        "        Q = Q.reshape(batch_size, query_len, self.nhead, self.head_dim).transpose(1, 2)\n",
        "        K = K.reshape(batch_size, key_len, self.nhead, self.head_dim).transpose(1, 2)\n",
        "        V = V.reshape(batch_size, value_len, self.nhead, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Q: [batch_size, nhead, query_len, head_dim]\n",
        "        # K: [batch_size, nhead, key_len, head_dim]\n",
        "        # V: [batch_size, nhead, value_len, head_dim]\n",
        "\n",
        "        # attention weight 계산\n",
        "        weight = torch.matmul(Q, K.transpose(2, 3)) / np.sqrt(self.head_dim)\n",
        "\n",
        "        # weight: [batch_size, nhead, query_len, key_len]\n",
        "\n",
        "        if mask is not None:\n",
        "            # mask 가 true 면 -1e9로 하여 softmax 값 0으로.\n",
        "            weight = weight.masked_fill(mask, -1e9)\n",
        "\n",
        "        attention = torch.softmax(weight, dim=-1) # TODO : softmax -1?\n",
        "\n",
        "        # attention: [batch_size, nhead, query_len, key_len]\n",
        "\n",
        "        # context vector 계산\n",
        "        c = torch.matmul(self.dropout(attention), V)\n",
        "\n",
        "        # c: [batch_size, nhead, query_len, head_dim]\n",
        "\n",
        "        c = c.transpose(1, 2)\n",
        "\n",
        "        # c: [batch_size, query_len, nhead, head_dim]\n",
        "\n",
        "        # concat\n",
        "        c = c.reshape(batch_size, query_len, self.d_model)\n",
        "\n",
        "        # c: [batch_size, query_len, d_model]\n",
        "\n",
        "        output = self.oLinear(c)\n",
        "\n",
        "        # output: [batch_size, query_len, d_model]\n",
        "\n",
        "        return output, attention"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9mShNXXUXhh"
      },
      "source": [
        "# Position-wise Feed Forward Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBXPWolrUeYj"
      },
      "source": [
        "class PositionWiseFeedForwardLayer(nn.Module):\n",
        "    def __init__(self, d_model, ff_dim, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(d_model, ff_dim)\n",
        "        self.linear2 = nn.Linear(ff_dim, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # x: [batch_size, seq_len, d_model]\n",
        "\n",
        "        x = self.dropout(torch.relu(self.linear1(x)))\n",
        "\n",
        "        # x: [batch_size, seq_len, ff_dim]\n",
        "\n",
        "        x = self.linear2(x)\n",
        "\n",
        "        # x: [batch_size, seq_len, d_model]\n",
        "\n",
        "        return x"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4hDc3YiUgXZ"
      },
      "source": [
        "# Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTPD0jEbe1bx"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, ff_dim, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layerNorm1 = nn.LayerNorm(d_model)\n",
        "        self.layerNorm2 = nn.LayerNorm(d_model)\n",
        "        self.multiHeadAttentionLayer = MultiHeadAttentionLayer(d_model, nhead, dropout_ratio)\n",
        "        self.positionWiseFeedForward = PositionWiseFeedForwardLayer(d_model, ff_dim, dropout_ratio)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "\n",
        "        # src: [batch_size, src_len, d_model]\n",
        "        # src_mask: [batch_size, src_len]\n",
        "\n",
        "        # self attention 이므로 query, key, value 전부 동일\n",
        "        _src, _ = self.multiHeadAttentionLayer(src, src, src, src_mask)\n",
        "\n",
        "        # dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized\n",
        "        src = self.layerNorm1(src + self.dropout(_src))\n",
        "\n",
        "        # src: [batch_size, src_len, d_model]\n",
        "\n",
        "        _src = self.positionWiseFeedForward(src)\n",
        "\n",
        "        # dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized\n",
        "        src = self.layerNorm2(src + self.dropout(_src))\n",
        "\n",
        "        # src: [batch_size, src_len, d_model]\n",
        "\n",
        "        return src"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz9_mr_kUimN"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAd_op0bVhn-"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, n_layers, nhead, ff_dim, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        # EncoderLayer 을 n 번 반복.\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, nhead, ff_dim, dropout_ratio) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "\n",
        "        # src: [batch_size, src_len]\n",
        "        # src_mask: [batch_size, src_len]\n",
        "\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "\n",
        "        # src: [batch_size, src_len, d_model]\n",
        "\n",
        "        return src"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htBsuMIRUkBk"
      },
      "source": [
        "# Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jn4VCWdXhK5"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, ff_dim, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layerNorm1 = nn.LayerNorm(d_model)\n",
        "        self.layerNorm2 = nn.LayerNorm(d_model)\n",
        "        self.layerNorm3 = nn.LayerNorm(d_model)\n",
        "        self.multiHeadAttentionLayer1 = MultiHeadAttentionLayer(d_model, nhead, dropout_ratio)\n",
        "        self.multiHeadAttentionLayer2= MultiHeadAttentionLayer(d_model, nhead, dropout_ratio)\n",
        "        self.positionWiseFeedForward = PositionWiseFeedForwardLayer(d_model, ff_dim, dropout_ratio)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    def forward(self, tgt, enc_src, tgt_mask, src_mask):\n",
        "\n",
        "        # tgt: [batch_size, tgt_len, d_model]\n",
        "        # enc_src: [batch_size, src_len, d_model]\n",
        "        # tgt_mask: [batch_size, tgt_len]\n",
        "        # src_mask: [batch_size, src_len]\n",
        "\n",
        "        # self attention\n",
        "        _tgt, _ = self.multiHeadAttentionLayer1(tgt, tgt, tgt, tgt_mask)\n",
        "\n",
        "        # dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized\n",
        "        tgt = self.layerNorm1(tgt + self.dropout(_tgt))\n",
        "\n",
        "        # tgt: [batch_size, tgt_len, d_model]\n",
        "\n",
        "        # encoder attention\n",
        "        # decoder 의 query, encdoer 의 key, value 로 attention\n",
        "        _tgt, attention = self.multiHeadAttentionLayer2(tgt, enc_src, enc_src, src_mask)\n",
        "\n",
        "        # dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized\n",
        "        tgt = self.layerNorm2(tgt + self.dropout(_tgt))\n",
        "\n",
        "        # tgt: [batch_size, tgt_len, d_model]\n",
        "\n",
        "        # positionwise feedforward\n",
        "        _tgt = self.positionWiseFeedForward(tgt)\n",
        "\n",
        "        # dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized\n",
        "        tgt = self.layerNorm3(tgt + self.dropout(_tgt))\n",
        "\n",
        "        # tgt: [batch_size, tgt_len, d_model]\n",
        "        # attention: [batch_size, nhead, tgt_len, src_len]\n",
        "\n",
        "        return tgt, attention"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNGeYlLSUsKx"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X64at7IuWQcm"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, d_model, n_layers, nhead, ff_dim, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, nhead, ff_dim, dropout_ratio) for _ in range(n_layers)])\n",
        "\n",
        "        self.fc_out = nn.Linear(d_model, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    def forward(self, tgt, enc_src, tgt_mask, src_mask):\n",
        "\n",
        "        # tgt: [batch_size, tgt_len]\n",
        "        # enc_src: [batch_size, src_len, d_model]\n",
        "        # tgt_mask: [batch_size, tgt_len]\n",
        "        # src_mask: [batch_size, src_len]\n",
        "\n",
        "        batch_size = tgt.shape[0]\n",
        "        tgt_len = tgt.shape[1]\n",
        "\n",
        "        # tgt: [batch_size, tgt_len, d_model]\n",
        "\n",
        "        for layer in self.layers:\n",
        "            tgt, attention = layer(tgt, enc_src, tgt_mask, src_mask)\n",
        "\n",
        "        # tgt: [batch_size, tgt_len, d_model]\n",
        "        # attention: [batch_size, nhead, tgt_len, src_len]\n",
        "\n",
        "        output = self.fc_out(tgt)\n",
        "\n",
        "        # output: [batch_size, tgt_len, output_dim]\n",
        "\n",
        "        return output, attention"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H95PTxccUtnV"
      },
      "source": [
        "# Token & Positional Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daBXLIF27xkC"
      },
      "source": [
        "class TokPosEmbedding(nn.Module):\n",
        "  def __init__(self, input_dim, d_model, dropout_ratio):\n",
        "    super().__init__()\n",
        "    self.tokEmbedding = nn.Embedding(input_dim, d_model)\n",
        "    self.posEmbedding = nn.Embedding(input_dim, d_model) # 위치 임베딩을 고정 함수가 아니라 학습하는 형태로 구현.\n",
        "    self.d_model = d_model\n",
        "    self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "  def forward(self, src):\n",
        "    batch_size = src.shape[0]\n",
        "    src_len = src.shape[1]\n",
        "\n",
        "    pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(device) # 0 ~ src_len 등차 수열값 을 bactch 한개와 동일한 shape으로 생성\n",
        "    \n",
        "    # pos: [batch_size, src_len]\n",
        "\n",
        "    src = self.dropout((self.tokEmbedding(src) * np.sqrt(self.d_model)) + self.posEmbedding(pos))\n",
        "\n",
        "    # src: [batch_size, src_len, d_model]\n",
        "\n",
        "    return src"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEq9zN7rU1gn"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBGN8VyvW0Et"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, d_model, n_layers, nhead, ff_dim, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encEmbedding = TokPosEmbedding(input_dim, d_model, dropout_ratio)\n",
        "        self.encoder = Encoder(input_dim, d_model, n_layers, nhead, ff_dim, dropout_ratio)\n",
        "        self.decEmbedding = TokPosEmbedding(output_dim, d_model, dropout_ratio)\n",
        "        self.decoder = Decoder(output_dim, d_model, n_layers, nhead, ff_dim, dropout_ratio)\n",
        "\n",
        "    def make_src_mask(self, src, pad):\n",
        "\n",
        "        # src: [batch_size, src_len]\n",
        "\n",
        "        # pad mask\n",
        "        src_mask = (src.data.eq(pad)).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # src_mask: [batch_size, 1, 1, src_len]\n",
        "        return src_mask\n",
        "\n",
        "    def make_tgt_mask(self, tgt, pad):\n",
        "        \n",
        "        # tgt: [batch_size, tgt_len]\n",
        "\n",
        "        tgt_pad_mask = (tgt.data.eq(pad)).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # tgt_pad_mask: [batch_size, 1, 1, tgt_len]\n",
        "\n",
        "        tgt_len = tgt.shape[1]\n",
        "\n",
        "        # chitting 방지 mask\n",
        "        tgt_sub_mask = torch.triu(torch.ones((tgt_len, tgt_len))).bool().to(device)\n",
        "\n",
        "        # tgt_sub_mask: [tgt_len, tgt_len] (upper triangular square matrix 형태)\n",
        "\n",
        "        # mask 값이 true 면 fill 할 것이므로 pad_mask 와 or 연산.\n",
        "        tgt_mask = tgt_pad_mask | tgt_sub_mask\n",
        "\n",
        "        # tgt_mask: [batch_size, 1, tgt_len, tgt_len]\n",
        "\n",
        "        return tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt, pad):\n",
        "\n",
        "        # src: [batch_size, src_len]\n",
        "        # tgt: [batch_size, tgt_len]\n",
        "\n",
        "        src_mask = self.make_src_mask(src, pad)\n",
        "        tgt_mask = self.make_tgt_mask(tgt, pad)\n",
        "\n",
        "        # src_mask: [batch_size, 1, 1, src_len]\n",
        "        # tgt_mask: [batch_size, 1, tgt_len, tgt_len]\n",
        "\n",
        "        src = self.encEmbedding(src)\n",
        "        tgt = self.decEmbedding(tgt)\n",
        "\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "\n",
        "        # enc_src: [batch_size, src_len, d_model]\n",
        "\n",
        "        output, attention = self.decoder(tgt, enc_src, tgt_mask, src_mask)\n",
        "\n",
        "        # output: [batch_size, tgt_len, output_dim]\n",
        "        # attention: [batch_size, nhead, tgt_len, src_len]\n",
        "\n",
        "        return output, attention\n",
        "\n",
        "    def generate(self, src, start_id, sample_size, pad):\n",
        "\n",
        "        batch_size = src.shape[0]\n",
        "\n",
        "        src_mask = self.make_src_mask(src, pad)\n",
        "        src = self.encEmbedding(src)\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "      \n",
        "        sampled_tensor = torch.LongTensor([start_id]).unsqueeze(0).repeat(batch_size, 1).to(device)\n",
        "        for _ in range(sample_size):\n",
        "\n",
        "          tgt_mask = self.make_tgt_mask(sampled_tensor, pad)\n",
        "          tgt = self.decEmbedding(sampled_tensor)\n",
        "          probabilities, attention = self.decoder(tgt, enc_src, tgt_mask, src_mask)\n",
        "\n",
        "          pred_token = probabilities.argmax(2)[:,-1].unsqueeze(1)\n",
        "\n",
        "          sampled_tensor = torch.cat((sampled_tensor, pred_token), 1) # pred sentence 에 concat\n",
        "\n",
        "        return sampled_tensor, attention\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjn6o7KAU41P"
      },
      "source": [
        "# Date format Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsb2_dgXFLGm",
        "outputId": "dc39c37a-1653-4307-e54b-8af8f2d28569"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V20NCZ8sE45D",
        "outputId": "e7506db6-baea-45ca-ccf9-80017824e05d"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/dataset')\n",
        "import sequence\n",
        "\n",
        "# google mount 한 뒤 '/content/drive/MyDrive/' 에 dataset 저장 후 실행.\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
        "# char -> id & id -> char dictionary\n",
        "char_to_id, id_to_char = sequence.get_vocab()\n",
        "\n",
        "print(x_train.shape)\n",
        "print(t_train.shape)\n",
        "print(x_test.shape)\n",
        "print(t_test.shape)\n",
        "print()\n",
        "\n",
        "# 이미 id 화 되어있다.\n",
        "print('question(id) : ', x_train[0])\n",
        "print('correct(id)  : ',t_train[0])\n",
        "print()\n",
        "\n",
        "# sequence 확인 # ' ' : pad, '_' : start_char\n",
        "print('question(char) : ', ' '.join([id_to_char[int(c)] for c in x_train[0]]))\n",
        "print('correct(char)  : ', ' '.join([id_to_char[int(c)] for c in t_train[0]]))\n",
        "\n",
        "vocab_size = len(char_to_id)\n",
        "x_train = torch.Tensor(x_train)\n",
        "t_train = torch.Tensor(t_train)\n",
        "x_test = torch.Tensor(x_test)\n",
        "t_test = torch.Tensor(t_test)\n",
        "pad = 7 # pad token"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(45000, 29)\n",
            "(45000, 11)\n",
            "(5000, 29)\n",
            "(5000, 11)\n",
            "\n",
            "question(id) :  [ 8 22  9 22  9  8  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
            "  7  7  7  7  7]\n",
            "correct(id)  :  [14 11 12  9  8 15 16  8 15 16  9]\n",
            "\n",
            "question(char) :  2 / 7 / 7 2                                              \n",
            "correct(char)  :  _ 1 9 7 2 - 0 2 - 0 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Iig2J1FVKG2"
      },
      "source": [
        "# hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWu02slRFGUH"
      },
      "source": [
        "batch_size = 128\n",
        "epoch = 10\n",
        "input_dim = output_dim = vocab_size\n",
        "d_model = 32\n",
        "n_layers = 1\n",
        "nhead = 2\n",
        "ff_dim = 1024\n",
        "dropout_ratio = 0.1\n",
        "learning_rate = 0.0025"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVgG8VOYXbIk"
      },
      "source": [
        "model = Transformer(input_dim, output_dim, d_model, n_layers, nhead, ff_dim, dropout_ratio)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6-92JSlXrwM"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqsZHm5eVQOd"
      },
      "source": [
        "# Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leJgv-xaF_C3"
      },
      "source": [
        "def trainer(x, t, max_epoch, batch_size, model, optimizer, pad):\n",
        "\n",
        "    data_size = len(x)\n",
        "    max_iters = data_size // batch_size\n",
        "    pad = torch.Tensor([pad]).to(device=device, dtype=dtype)\n",
        "\n",
        "    model.train()\n",
        "    model = model.to(device)\n",
        "    for e in range(max_epoch):\n",
        "        for iters in range(max_iters):\n",
        "            batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
        "            batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
        "\n",
        "            batch_x = batch_x.to(device=device, dtype=dtype)\n",
        "            batch_t = batch_t.to(device=device, dtype=dtype)\n",
        "\n",
        "            # correct 값의 마지막 원소 배제 (end token 없음. 마지막 원소 다음 token 학습할 필요 x)\n",
        "            scores, _ = model(batch_x, batch_t[:, :-1], pad)\n",
        "\n",
        "            scores_dim = scores.shape[-1]\n",
        "\n",
        "            scores = scores.reshape(-1, scores_dim)\n",
        "            \n",
        "            # correct 값의 첫 원소 배제\n",
        "            batch_t = batch_t[:, 1:].reshape(-1)\n",
        "\n",
        "            # output: [batch_size * tgt_len - 1, output_dim]\n",
        "            # tgt: [batch_size * tgt len - 1]\n",
        "\n",
        "            loss = criterion(scores, batch_t)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0) # gradient clippling\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            if iters % 100 == 0:\n",
        "                print('epoch[%d/%d] Iteration %d/%d, loss = %.4f' % (e+1, max_epoch, iters, max_iters, loss.item()))"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5DUs38oVT0V"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRJTQ6YgF_4D",
        "outputId": "a229bbb7-924c-4df6-a230-9c3ab317cd5d"
      },
      "source": [
        "trainer(x_train, t_train, epoch, batch_size, model, optimizer, pad)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch[1/10] Iteration 0/351, loss = 4.0747\n",
            "epoch[1/10] Iteration 100/351, loss = 0.9148\n",
            "epoch[1/10] Iteration 200/351, loss = 0.7019\n",
            "epoch[1/10] Iteration 300/351, loss = 0.4790\n",
            "epoch[2/10] Iteration 0/351, loss = 0.4312\n",
            "epoch[2/10] Iteration 100/351, loss = 0.3390\n",
            "epoch[2/10] Iteration 200/351, loss = 0.2744\n",
            "epoch[2/10] Iteration 300/351, loss = 0.2009\n",
            "epoch[3/10] Iteration 0/351, loss = 0.2024\n",
            "epoch[3/10] Iteration 100/351, loss = 0.2024\n",
            "epoch[3/10] Iteration 200/351, loss = 0.1782\n",
            "epoch[3/10] Iteration 300/351, loss = 0.1481\n",
            "epoch[4/10] Iteration 0/351, loss = 0.1378\n",
            "epoch[4/10] Iteration 100/351, loss = 0.1610\n",
            "epoch[4/10] Iteration 200/351, loss = 0.1247\n",
            "epoch[4/10] Iteration 300/351, loss = 0.1125\n",
            "epoch[5/10] Iteration 0/351, loss = 0.0755\n",
            "epoch[5/10] Iteration 100/351, loss = 0.0996\n",
            "epoch[5/10] Iteration 200/351, loss = 0.1002\n",
            "epoch[5/10] Iteration 300/351, loss = 0.1182\n",
            "epoch[6/10] Iteration 0/351, loss = 0.0895\n",
            "epoch[6/10] Iteration 100/351, loss = 0.0989\n",
            "epoch[6/10] Iteration 200/351, loss = 0.0776\n",
            "epoch[6/10] Iteration 300/351, loss = 0.0764\n",
            "epoch[7/10] Iteration 0/351, loss = 0.0756\n",
            "epoch[7/10] Iteration 100/351, loss = 0.0962\n",
            "epoch[7/10] Iteration 200/351, loss = 0.0593\n",
            "epoch[7/10] Iteration 300/351, loss = 0.0691\n",
            "epoch[8/10] Iteration 0/351, loss = 0.0658\n",
            "epoch[8/10] Iteration 100/351, loss = 0.0660\n",
            "epoch[8/10] Iteration 200/351, loss = 0.0819\n",
            "epoch[8/10] Iteration 300/351, loss = 0.0570\n",
            "epoch[9/10] Iteration 0/351, loss = 0.0635\n",
            "epoch[9/10] Iteration 100/351, loss = 0.0698\n",
            "epoch[9/10] Iteration 200/351, loss = 0.0872\n",
            "epoch[9/10] Iteration 300/351, loss = 0.0594\n",
            "epoch[10/10] Iteration 0/351, loss = 0.0515\n",
            "epoch[10/10] Iteration 100/351, loss = 0.0532\n",
            "epoch[10/10] Iteration 200/351, loss = 0.0575\n",
            "epoch[10/10] Iteration 300/351, loss = 0.0451\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeSc5TuMVVcy"
      },
      "source": [
        "# Checker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07xS7V5WR5Hk"
      },
      "source": [
        "def checker(x, t, batch_size, model, pad):\n",
        "\n",
        "    data_size = len(x)\n",
        "    max_iters = data_size // batch_size\n",
        "    pad = torch.Tensor([pad]).to(device=device, dtype=dtype)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      correct_num = 0\n",
        "      for iters in range(max_iters):\n",
        "        batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
        "        batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
        "\n",
        "        batch_x = batch_x.to(device=device, dtype=dtype)\n",
        "        batch_t = batch_t.to(device=device, dtype=dtype)\n",
        "\n",
        "        start_id = batch_t[0,0]\n",
        "        correct = batch_t[:,1:]\n",
        "\n",
        "        predict, _ = model.generate(batch_x, start_id, correct.shape[1], pad)\n",
        "        predict = predict[:,1:]\n",
        "\n",
        "        correct_num += (predict == correct).sum()\n",
        "        \n",
        "    return correct_num"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1x5iY43VZIY"
      },
      "source": [
        "# Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wdj6iQCfFWmU",
        "outputId": "33807903-57b7-4910-e1c3-75ce8a88ecb3"
      },
      "source": [
        "correct_num = checker(x_test, t_test, batch_size, model, pad)\n",
        "acc = float(correct_num) / (t_test.shape[0] * (t_test.shape[1] - 1))\n",
        "print('accuracy %.3f%%' % (acc * 100))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy 97.598%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53RqchO4Vgbs"
      },
      "source": [
        "# Visualizing Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj9G0PpRV4Vl"
      },
      "source": [
        "def visualize(attention_map, row_labels, column_labels):\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.pcolor(attention_map, cmap=plt.cm.Greys_r, vmin=0.0, vmax=1.0)\n",
        "\n",
        "    ax.patch.set_facecolor('black')\n",
        "    ax.set_yticks(np.arange(attention_map.shape[0])+0.5, minor=False)\n",
        "    ax.set_xticks(np.arange(attention_map.shape[1])+0.5, minor=False)\n",
        "    ax.invert_yaxis()\n",
        "    ax.set_xticklabels(row_labels, minor=False)\n",
        "    ax.set_yticklabels(column_labels, minor=False)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMSpb_31Xa6G"
      },
      "source": [
        "## (x, y) = (question, predict X nhead)\n",
        "### 학습 후 반복적으로 실행시켜 attention 이 잘 작동하는 것을 확인할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "GqHRrpcPV7OM",
        "outputId": "1df89fa2-af6a-4c3a-dae9-642bf4230896"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "idx = [np.random.randint(0, len(x_test))]\n",
        "\n",
        "question = x_test[idx].to(device, dtype)\n",
        "correct = t_test[idx].to(device, dtype)\n",
        "pad = torch.Tensor([pad]).to(device=device, dtype=dtype)\n",
        "\n",
        "correct = correct.flatten()\n",
        "# 머릿글자\n",
        "start_id = correct[0]\n",
        "\n",
        "correct = correct[1:]\n",
        "with torch.no_grad():\n",
        "  predict, attention  = model.generate(question, start_id, len(correct), pad)\n",
        "predict = predict[:,1:]\n",
        "\n",
        "# 문자열로 변환\n",
        "question = [id_to_char[int(c)] for c in question.flatten()]\n",
        "correct = [id_to_char[int(c)] for c in correct.flatten()]\n",
        "predict = [id_to_char[int(c)] for c in predict.flatten()]\n",
        "\n",
        "for h in range(nhead):\n",
        "  visualize(attention[0, h].cpu().numpy(), question, predict)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}